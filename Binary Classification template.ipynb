{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Classification template\n",
    "\n",
    "* Import Packages\n",
    "* Data Exploration\n",
    "    * Summary of the original dataset\n",
    "    * Create new features\n",
    "    * Missing values\n",
    "    * Dataset normalization\n",
    "    * (Dataset PCA)\n",
    "    * Visulization \n",
    "        * Feature correlation heatmap\n",
    "        * Certain feature histograms with target variable as hue\n",
    "        * kde plot\n",
    "* Modeling\n",
    "    * Feature selection\n",
    "        * Univariate Feature selection\n",
    "        * Decision tree feature importance\n",
    "        * RFE\n",
    "    * Train test datasets split (Cross validation)\n",
    "    * Build model\n",
    "        * Logistic regression\n",
    "        * Decission regression\n",
    "        * Random Forest \n",
    "        * Gradient Boosting\n",
    "        * SVM\n",
    "    * Model evaluation on both training, test, and validation datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# General settings\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Display all plots inline in Jupyter notebook\n",
    "%matplotlib inline\n",
    "#set 'png' here when working on notebook\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Visualisation packages\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pylab as pylab\n",
    "import seaborn as sns\n",
    "\n",
    "# Configure visualizations\n",
    "mpl.style.use('ggplot')\n",
    "#sns.set_style('white')\n",
    "pylab.rcParams['figure.figsize'] = 8,6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import pandas and numpy packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# Avoid truncate the display\n",
    "#pd.options.display.max_rows =2000\n",
    "#pd.options.display.max_columns=2000\n",
    "\n",
    "import cPickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import sklearn\n",
    "# Modelling Algorithms\n",
    "import sklearn\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC,LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "\n",
    "# Modelling Helpers\n",
    "from sklearn.preprocessing import Imputer, Normalizer, scale, StandardScaler, MinMaxScaler\n",
    "from sklearn.cross_validation import train_test_split, KFold, StratifiedKFold\n",
    "from sklearn.grid_search import GridSearchCV #for tunning hyper parameter it will use all combination of of given parameters\n",
    "from sklearn.grid_search import RandomizedSearchCV # same for tunning hyper parameter but will use random combinations of parameters\n",
    "from sklearn.feature_selection import RFECV,RFE,SelectPercentile,f_classif\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score,recall_score, precision_recall_curve,auc, roc_curve, roc_auc_score, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration\n",
    "\n",
    "* Summary of the original dataset\n",
    "* Create new features\n",
    "* Handle missing values\n",
    "* Dataset normalization\n",
    "* (Dataset PCA)\n",
    "* Visulization \n",
    "    * Feature correlation heatmap\n",
    "    * Certain feature histograms with target variable as hue\n",
    "    * kde plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-340ae47a8977>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Summary of the original dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescribe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# Summary of the original dataset\n",
    "df.describe()\n",
    "df.shape()\n",
    "df.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create new features\n",
    "# x.apply\n",
    "# lamda function\n",
    "# Binary columns using np.where, one-hot encoding, dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Handle missing values\n",
    "df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dataset normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dataset PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Visulization\n",
    "sns.heatmap(df.corr(),xticklabels=df.columns(),yticklabels=df.columns())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling\n",
    "\n",
    "* Feature selection\n",
    "    * Univariate Feature selection\n",
    "    * Decision tree feature importance\n",
    "    * RFE\n",
    "* Train test datasets split (Cross validation)\n",
    "* Build model\n",
    "    * Logistic regression\n",
    "    * Decission regression\n",
    "    * Random Forest \n",
    "    * Gradient Boosting\n",
    "    * SVM\n",
    "* Model evaluation on both training, test, and validation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Feature selection using randomeforestclassifier feature importance\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "def feature_tree_imp(X,y):\n",
    "    tree = RandomForestClassifier()\n",
    "    tree.fit(X, y)\n",
    "    imp = pd.DataFrame(tree.feature_importances_, columns = [ 'Importance' ], index = X.columns)\n",
    "    imp = imp.sort_values( ['Importance'], ascending = False)\n",
    "    return imp\n",
    "\n",
    "train_X,test_X,train_y,test_y= train_test_split(X, y,test_size=0.3, \\\n",
    "                                                random_state=42,stratify=y)\n",
    "imp_test = feature_tree_imp(train_X, train_y)\n",
    "print imp_test\n",
    "\n",
    "imp_feature_list = list(imp_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use RFE or RFECV to find the best features\n",
    "# use logistic regression here\n",
    "\n",
    "clf = LogisticRegression()\n",
    "rfe = RFE(clf, 8)\n",
    "rfe = rfe.fit(X,y)\n",
    "# summarize the selection of the attributes\n",
    "print('Selected features: %s' % list(X.columns[rfe.support_]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "selected_features = ['','','','']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFECV\n",
    "# Create the RFE object and compute a cross-validated score.\n",
    "# The \"accuracy\" scoring is proportional to the number of correct classifications\n",
    "rfecv = RFECV(estimator=LogisticRegression(), step=1, cv=10, scoring='accuracy')\n",
    "rfecv.fit(X, y)\n",
    "\n",
    "print(\"Optimal number of features: %d\" % rfecv.n_features_)\n",
    "print('Selected features: %s' % list(X.columns[rfecv.support_]))\n",
    "\n",
    "# Plot number of features VS. cross-validation scores\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.xlabel(\"Number of features selected\")\n",
    "plt.ylabel(\"Cross validation score (nb of correct classifications)\")\n",
    "plt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train test datasets split (Cross validation)\n",
    "train_X,test_X,train_y,test_y= train_test_split(X[imp_feature_list[0:15]],y,test_size=0.3,\\\n",
    "                                                random_state=42,stratify=y_predict_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build models\n",
    "clf= LogisticRegression(class_weight='balanced', random_state = 0)\n",
    "#clf= LogisticRegression(penalty = 'l1', C = 0.00001,class_weight='balanced', random_state = 0)\n",
    "#clf= LogisticRegression(penalty = 'l1', C = 0.00001, random_state = 0)\n",
    "#clf= LogisticRegression(penalty = 'l2', C = 10000, class_weight='balanced',random_state = 0)\n",
    "# For unbalaced dataset, RF might be not a good option, needs to manually oversample some class\n",
    "#clf = GaussianNB()\n",
    "#clf = DecisionTreeClassifier(max_depth=15)\n",
    "#clf = RandomForestClassifier(random_state=0, n_estimators=50,max_depth=10)\n",
    "#clf = RandomForestClassifier()\n",
    "\n",
    "clf.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model evaluation on both training, test, and validation datasets\n",
    "\n",
    "# Get prediction\n",
    "pred = clf.predict(test_X)\n",
    "predprob = clf.predict_proba(test_X)\n",
    "\n",
    "# Evaluation\n",
    "cnf_matrix = confusion_matrix(test_y, pred)\n",
    "TP = cnf_matrix[1, 1]\n",
    "TN = cnf_matrix[0, 0]\n",
    "FP = cnf_matrix[0, 1]\n",
    "FN = cnf_matrix[1, 0]\n",
    "accuracy = accuracy_score(test_y, pred)\n",
    "recall = recall_score(test_y, pred)\n",
    "specificity = TN / float(TN + FP)\n",
    "auc = roc_auc_score(test_y, predprob[:, 1])\n",
    "\n",
    "print \"TP:\", TP  # no of fraud transaction which are predicted fraud\n",
    "print \"TN:\", TN  # no. of normal transaction which are predited normal\n",
    "print \"FP:\", FP  # no of normal transaction which are predicted fraud\n",
    "print \"FN:\", FN, \"\\n\"  # no of fraud Transaction which are predicted normal\n",
    "print cnf_matrix, \"\\n\"\n",
    "print \"Original non-redemption distribution:\", 1 - len(test_y[test_y == 1]) / (len(test_y) * 1.0)\n",
    "print \"Classification Accuracy:\", accuracy\n",
    "print \"Recall:\", recall\n",
    "print \"Specificity:\", specificity\n",
    "print \"AUC:\", auc\n",
    "print(\"\\n----------Classification Report on dataset-------------\")\n",
    "print(classification_report(test_y, pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
